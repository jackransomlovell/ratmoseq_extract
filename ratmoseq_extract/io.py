import numpy as np
import cv2
import subprocess
from tqdm import tqdm
import datetime
import matplotlib.pyplot as plt

def write_extracted_chunk_to_h5(
    h5_file, results, config_data, scalars, frame_range, offset
):
    """

    Write extracted frames, frame masks, and scalars to an open h5 file.

    Args:
    h5_file (H5py.File): open results_00 h5 file to save data in.
    results (dict): extraction results dict.
    config_data (dict): dictionary containing extraction parameters (autogenerated)
    scalars (list): list of keys to scalar attribute values
    frame_range (range object): current chunk frame range
    offset (int): frame offset

    Returns:
    """

    # Writing computed scalars to h5 file
    for scalar in scalars:
        h5_file[f"scalars/{scalar}"][frame_range] = results["scalars"][scalar][offset:]

    # Writing frames and mask to h5
    h5_file["frames"][frame_range] = results["depth_frames"][offset:]
    h5_file["frames_mask"][frame_range] = results["mask_frames"][offset:]

    # Writing flip classifier results to h5
    if config_data["flip_classifier"]:
        h5_file["metadata/extraction/flips"][frame_range] = results["flips"][offset:]


def make_output_movie(results, config_data, offset=0):
    """
    Create an array for output movie with filtered video and cropped mouse on the top left

    Args:
    results (dict): dict of extracted depth frames, and original raw chunk to create an output movie.
    config_data (dict): dict of extraction parameters containing the crop sizes used in the extraction.
    offset (int): current offset being used, automatically set if chunk_overlap > 0

    Returns:
    output_movie (numpy.ndarray): output movie to write to mp4 file.
    """

    # Create empty array for output movie with filtered video and cropped mouse on the top left
    nframes, rows, cols = results["chunk"][offset:].shape
    output_movie = np.zeros(
        (
            nframes,
            rows + config_data["crop_size"][0],
            cols + config_data["crop_size"][1],
        ),
        "uint16",
    )

    # Populating array with filtered and cropped videos
    output_movie[:, : config_data["crop_size"][0], : config_data["crop_size"][1]] = (
        results["depth_frames"][offset:]
    )
    output_movie[:, config_data["crop_size"][0] :, config_data["crop_size"][1] :] = (
        results["chunk"][offset:]
    )

    # normalize from 0-255 for writing to mp4
    output_movie = (output_movie / output_movie.max() * 255).astype("uint8")

    return output_movie

def write_frames_preview(
    filename,
    frames=np.empty((0,)),
    threads=6,
    fps=30,
    pixel_format="rgb24",
    codec="h264",
    slices=24,
    slicecrc=1,
    frame_size=None,
    depth_min=0,
    depth_max=80,
    get_cmd=False,
    cmap="jet",
    pipe=None,
    close_pipe=True,
    frame_range=None,
    progress_bar=False,
):
    """
    Simple command to pipe frames to an ffv1 file. Writes out a false-colored mp4 video.

    Args:
    filename (str): path to file to write to.
    frames (np.ndarray): frames to write
    threads (int): number of threads to write video
    fps (int): frames per second
    pixel_format (str): format video color scheme
    codec (str): ffmpeg encoding-writer method to use
    slices (int): number of frame slices to write at a time.
    slicecrc (int): check integrity of slices
    frame_size (tuple): shape/dimensions of image.
    depth_min (int): minimum mouse depth from floor in (mm)
    depth_max (int): maximum mouse depth from floor in (mm)
    get_cmd (bool): indicates whether function should return ffmpeg command (instead of executing)
    cmap (str): color map to use.
    pipe (subProcess.Pipe): pipe to currently open video file.
    close_pipe (bool): indicates to close the open pipe to video when done writing.
    frame_range (range()): frame indices to write on video
    progress_bar (bool): If True, displays a TQDM progress bar for the video writing progress.

    Returns:
    pipe (subProcess.Pipe): indicates whether video writing is complete.
    """

    font = cv2.FONT_HERSHEY_SIMPLEX
    white = (255, 255, 255)
    txt_pos = (5, frames.shape[-1] - 40)

    if not np.mod(frames.shape[1], 2) == 0:
        frames = np.pad(frames, ((0, 0), (0, 1), (0, 0)), "constant", constant_values=0)

    if not np.mod(frames.shape[2], 2) == 0:
        frames = np.pad(frames, ((0, 0), (0, 0), (0, 1)), "constant", constant_values=0)

    if not frame_size and type(frames) is np.ndarray:
        frame_size = "{0:d}x{1:d}".format(frames.shape[2], frames.shape[1])
    elif not frame_size and type(frames) is tuple:
        frame_size = "{0:d}x{1:d}".format(frames[0], frames[1])

    command = [
        "ffmpeg",
        "-y",
        "-loglevel",
        "fatal",
        "-threads",
        str(threads),
        "-framerate",
        str(fps),
        "-f",
        "rawvideo",
        "-s",
        frame_size,
        "-pix_fmt",
        pixel_format,
        "-i",
        "-",
        "-an",
        "-vcodec",
        codec,
        "-slices",
        str(slices),
        "-slicecrc",
        str(slicecrc),
        "-r",
        str(fps),
        filename,
    ]

    if get_cmd:
        return command

    if not pipe:
        pipe = subprocess.Popen(command, stdin=subprocess.PIPE, stderr=subprocess.PIPE)

    # scale frames to appropriate depth ranges
    use_cmap = plt.get_cmap(cmap)
    for i in tqdm(
        range(frames.shape[0]),
        disable=not progress_bar,
        desc=f"Writing frames to {filename}",
    ):
        disp_img = frames[i, :].copy().astype("float32")
        disp_img = (disp_img - depth_min) / (depth_max - depth_min)
        disp_img[disp_img < 0] = 0
        disp_img[disp_img > 1] = 1
        disp_img = np.delete(use_cmap(disp_img), 3, 2) * 255
        if frame_range is not None:
            try:
                cv2.putText(
                    disp_img,
                    str(frame_range[i]),
                    txt_pos,
                    font,
                    1,
                    white,
                    2,
                    cv2.LINE_AA,
                )
            except (IndexError, ValueError):
                # len(frame_range) M < len(frames) or
                # txt_pos is outside of the frame dimensions
                print("Could not overlay frame number on preview on video.")

        pipe.stdin.write(disp_img.astype("uint8").tostring())

    if close_pipe:
        pipe.communicate()
        return None
    else:
        return pipe
    
def read_frames(
    filename,
    frames=range(
        0,
    ),
    threads=6,
    fps=30,
    frames_is_timestamp=False,
    pixel_format="gray16le",
    movie_dtype="uint16",
    frame_size=None,
    slices=24,
    slicecrc=1,
    mapping="DEPTH",
    get_cmd=False,
    finfo=None,
    **kwargs,
):
    """
    Read in frames from the .mp4/.avi file using a pipe from ffmpeg.

    Args:
    filename (str): filename to get frames from
    frames (list or numpy.ndarray): list of frames to grab
    threads (int): number of threads to use for decode
    fps (int): frame rate of camera
    frames_is_timestamp (bool): if False, indicates timestamps represent kinect v2 absolute machine timestamps,
    pixel_format (str): ffmpeg pixel format of data
    movie_dtype (str): An indicator for numpy to store the piped ffmpeg-read video in memory for processing.
    frame_size (str): wxh frame size in pixels
    slices (int): number of slices to use for decode
    slicecrc (int): check integrity of slices
    mapping (str): the stream to read from mkv files.
    get_cmd (bool): indicates whether function should return ffmpeg command (instead of executing).
    finfo (dict): dictionary containing video file metadata

    Returns:
    video (numpy.ndarray):  frames x rows x columns
    """

    if finfo is None:
        finfo = get_video_info(filename, threads=threads, **kwargs)

    if frames is None or len(frames) == 0:
        frames = np.arange(finfo["nframes"], dtype="int64")

    if not frame_size:
        frame_size = finfo["dims"]

    # Compute starting time point to retrieve frames from
    if frames_is_timestamp:
        start_time = str(datetime.timedelta(seconds=frames[0]))
    else:
        start_time = str(datetime.timedelta(seconds=frames[0] / fps))

    command = [
        "ffmpeg",
        "-loglevel",
        "fatal",
        "-ss",
        start_time,
        "-i",
        filename,
        "-vframes",
        str(len(frames)),
        "-f",
        "image2pipe",
        "-s",
        "{:d}x{:d}".format(frame_size[0], frame_size[1]),
        "-pix_fmt",
        pixel_format,
        "-threads",
        str(threads),
        "-slices",
        str(slices),
        "-slicecrc",
        str(slicecrc),
        "-vcodec",
        "rawvideo",
    ]

    if isinstance(mapping, str):
        mapping_dict = get_stream_names(filename)
        mapping = mapping_dict.get(mapping, 0)

    if filename.endswith((".mkv", ".avi")):
        command += ["-map", f"0:{mapping}"]
        command += ["-vsync", "0"]

    command += ["-"]

    if get_cmd:
        return command

    pipe = subprocess.Popen(command, stderr=subprocess.PIPE, stdout=subprocess.PIPE)
    out, err = pipe.communicate()

    if err:
        print("Error:", err)
        return None

    video = np.frombuffer(out, dtype=movie_dtype).reshape(
        (len(frames), frame_size[1], frame_size[0])
    )

    return video.astype("uint16")

def get_stream_names(filename, stream_tag="title"):
    """
    Run an FFProbe command to determine whether an input video file contains multiple streams, and
    returns a stream_name to paired int values to extract the desired stream.

    Args:
    filename (str): path to video file to get streams from.
    stream_tag (str): value of the stream tags for ffprobe command to return

    Returns:
    out (dict): Dictionary of string to int pairs for the included streams in the mkv file.
    Dict will be used to choose the correct mapping number to choose which stream to read in read_frames().
    """

    command = [
        "ffprobe",
        "-v",
        "fatal",
        "-show_entries",
        "stream_tags={}".format(stream_tag),
        "-of",
        "default=noprint_wrappers=1:nokey=1",
        filename,
    ]

    ffmpeg = subprocess.Popen(command, stderr=subprocess.PIPE, stdout=subprocess.PIPE)
    out, err = ffmpeg.communicate()

    if err or len(out) == 0:
        return {"DEPTH": 0}

    out = out.decode("utf-8").rstrip("\n").split("\n")

    return {o: i for i, o in enumerate(out)}

def get_video_info(filename, mapping="DEPTH", threads=8, count_frames=False, **kwargs):
    """
    Get file metadata from videos.

    Args:
    filename (str): name of file to read video metadata from.
    mapping (str): chooses the stream to read from files.
    threads (int): number of threads to simultanoues run the ffprobe command
    count_frames (bool): indicates whether to count the frames individually.

    Returns:
    out_dict (dict): dictionary containing video file metadata
    """

    mapping_dict = get_stream_names(filename)
    if isinstance(mapping, str):
        mapping = mapping_dict.get(mapping, 0)

    stream_str = "stream=width,height,r_frame_rate,"
    if count_frames:
        stream_str += "nb_read_frames"
    else:
        stream_str += "nb_frames"

    command = [
        "ffprobe",
        "-v",
        "fatal",
        "-select_streams",
        f"v:{mapping}",
        "-show_entries",
        stream_str,
        "-of",
        "default=noprint_wrappers=1:nokey=1",
        "-threads",
        str(threads),
        filename,
        "-sexagesimal",
    ]

    if count_frames:
        command += ["-count_frames"]

    ffmpeg = subprocess.Popen(command, stderr=subprocess.PIPE, stdout=subprocess.PIPE)
    out, err = ffmpeg.communicate()

    if err:
        print(err)

    out = out.decode().split("\n")
    out_dict = {
        "file": filename,
        "dims": (int(float(out[0])), int(float(out[1]))),
        "fps": float(out[2].split("/")[0]) / float(out[2].split("/")[1]),
    }

    try:
        out_dict["nframes"] = int(out[3])
    except ValueError:
        out_dict["nframes"] = None

    return out_dict
