"""
Extraction helper utility for computing scalar feature values performing cleaning, cropping and rotating operations.
"""

import cv2
import numpy as np
from copy import deepcopy
from tqdm import tqdm
from pathlib import Path
import uuid
import os
from os.path import join, exists, abspath, basename, dirname
from os import makedirs, system
import warnings
from ruamel.yaml import YAML
import numpy as np
import h5py


from ratmoseq_extract.sam2 import get_sam2_predictor, segment_chunk, load_dlc
from ratmoseq_extract.proc import (
    crop_and_rotate_frames,
    threshold_chunk,
    get_bground,
    get_frame_features,
    get_flips,
    compute_scalars,
)

from ratmoseq_extract.io import (
    write_extracted_chunk_to_h5,
    make_output_movie,
    # load_movie_data,
    read_frames,
    write_frames_preview,
    handle_extract_metadata,
    get_movie_info,
    get_frame_range_indices,
    scalar_attributes,
    gen_batch_sequence,
    create_extract_h5,
    read_yaml,
    filter_warnings,
)

yaml = YAML(typ='safe', pure=True)

def process_extract_batches(
    input_file,
    config_data,
    bground_im,
    roi,
    frame_batches,
    # str_els,
    output_mov_path,
    scalars=None,
    h5_file=None,
    video_pipe=None,
    **kwargs,
):
    """
    Compute extracted frames and save them to h5 files and avi files.

    Args:
    input_file (str): path to depth file
    config_data (dict): dictionary containing extraction parameters (autogenerated)
    bground_im (numpy.ndarray):  background image
    roi (numpy.ndarray): roi image
    frame_batches (list): list of batches of frames to serially process.
    str_els (dict): dictionary containing OpenCV StructuringElements
    output_mov_path (str): path and filename of the output movie generated by the extraction
    scalars (list): list of keys to scalar attribute values
    h5file (h5py.File): opened h5 file to write extracted batches to
    video_pipe (subprocess.PIPE): open pipe to location where preview extraction is being written.
    kwargs (dict): Extra keyword arguments.

    Returns:
    """
    tmp_pdir = Path(output_mov_path).parent
    for i, frame_range in enumerate(tqdm(frame_batches, desc="Processing batches")):
        # raw_chunk = load_movie_data(
        #     input_file, frame_range, frame_size=bground_im.shape[::-1], **config_data
        # )
        raw_chunk = read_frames(input_file, frame_range, **config_data)
        tmp_save = tmp_pdir / f"raw_chunk_{i}.npy"
        np.save(str(tmp_save), raw_chunk)
        offset = config_data["chunk_overlap"] if i > 0 else 0

        # load DLC keypoints if available
        if config_data["dlc_filename"]:
            # get keypoints and bodyparts from config
            csv = Path(input_file).parents[0] / config_data["dlc_filename"]
            # load DLC data
            sam2_points = load_dlc(csv, frame_range)
            # add to config_data
            config_data["sam2_points"] = sam2_points

        # Get crop-rotated frame batch
        results = extract_chunk(
            **config_data, chunk=raw_chunk, roi=roi, bground=bground_im
        )

        # Offsetting frame chunk by CLI parameter defined option: chunk_overlap
        frame_range = frame_range[offset:]

        if h5_file is not None:
            write_extracted_chunk_to_h5(
                h5_file, results, config_data, scalars, frame_range, offset
            )

        # Create array for output movie with filtered video and cropped mouse on the top left
        output_movie = make_output_movie(results, config_data, offset)

        # Writing frame batch to mp4 file
        video_pipe = write_frames_preview(
            output_mov_path,
            output_movie,
            pipe=video_pipe,
            close_pipe=False,
            fps=config_data["fps"],
            frame_range=list(frame_range),
            depth_max=config_data["max_height"],
            depth_min=config_data["min_height"],
            progress_bar=config_data.get("progress_bar", False),
        )

    # Check if video is done writing. If not, wait.
    if video_pipe is not None:
        video_pipe.communicate()


def check_completion_status(status_filename):
    """
    Read a results_00.yaml (status file) and checks whether the session has been
    fully extracted.

    Args:
    status_filename (str): path to results_00.yaml

    Returns:
    complete (bool): If True, data has been extracted to completion.
    """

    if exists(status_filename):
        return read_yaml(status_filename)["complete"]
    return False


def run_extraction(input_file, config_data):
    """
    Extract depth videos.

    Args:
    input_file (str): path to depth file
    config_data (dict): dictionary containing extraction parameters.

    Returns:
    output_dir (str): path to directory containing extraction
    """
    print("Processing:", input_file)
    # get the basic metadata

    # ensure 'get_cmd' and 'run_cmd' are not in config_data or get_bground_im_file will fail
    config_data = {
        k: v
        for k, v in config_data.items()
        if k not in ("get_cmd", "run_cmd", "extensions")
    }

    status_dict = {
        "complete": False,
        "skip": False,
        "uuid": str(uuid.uuid4()),
        "metadata": "",
        "parameters": deepcopy(config_data),
    }

    # save input directory path
    in_dirname = dirname(input_file)

    # If input file is compressed (tarFile), returns decompressed file path and tar bool indicator.
    # Also gets loads respective metadata dictionary and timestamp array.
    acquisition_metadata, config_data["timestamps"], config_data["tar"] = (
        handle_extract_metadata(input_file, in_dirname)
    )

    # updating input_file reference to open tar file object if input file ends with [.tar/.tar.gz]
    if config_data["tar"] is not None:
        input_file = config_data["tar"]

    config_data["finfo"] = get_movie_info(input_file, **config_data)

    if config_data["finfo"]["nframes"] is None:
        config_data["finfo"]["nframes"] = len(config_data["timestamps"])

    status_dict["metadata"] = acquisition_metadata  # update status dict

    # Getting number of frames to extract
    if config_data['num_frames'] is None:
        nframes = int(config_data["finfo"]["nframes"])
    elif config_data['num_frames'] > config_data["finfo"]["nframes"]:
        warnings.warn(
            "Requested more frames than video includes, extracting whole recording..."
        )
        nframes = int(config_data["finfo"]["nframes"])
    elif isinstance(config_data['num_frames'], int):
        nframes = config_data['num_frames']

    # config_data = check_filter_sizes(config_data)

    # Compute total number of frames to include from an initial starting point.
    total_frames, first_frame_idx, last_frame_idx = get_frame_range_indices(
        *config_data["frame_trim"], nframes
    )

    scalars_attrs = scalar_attributes()
    scalars = list(scalars_attrs)

    # Get frame chunks to extract
    frame_batches = gen_batch_sequence(
        last_frame_idx,
        config_data["chunk_size"],
        config_data["chunk_overlap"],
        offset=first_frame_idx,
    )

    output_dir = config_data['outputdir']
    # set up the output directory
    if config_data['outputdir'] is None:
        output_dir = join(in_dirname, "proc")
    else:
        if in_dirname not in output_dir:
            output_dir = join(in_dirname, output_dir)

    if not exists(output_dir):
        os.makedirs(output_dir)

    output_filename = f'results_00'
    status_filename = join(output_dir, f"{output_filename}.yaml")
    movie_filename = join(output_dir, f"{output_filename}.mp4")
    results_filename = join(output_dir, f"{output_filename}.h5")

    with open(status_filename, "w") as f:
        yaml.dump(status_dict, f)

    # Get Structuring Elements for extraction
    # str_els = get_strels(config_data)

    roi = np.ones(config_data["finfo"]["dims"][::-1], dtype=np.uint8)
    bground_im, first_frame = get_bground(
        input_file, config_data, output_dir=output_dir
    )
    # Debugging option: DTD has no effect on extraction results unless dilate iterations > 1
    if config_data.get("detected_true_depth", "auto") == "auto":
        config_data["true_depth"] = np.median(bground_im[roi > 0])
    else:
        config_data["true_depth"] = int(config_data["detected_true_depth"])

    print("Detected true depth:", config_data["true_depth"])

    extraction_data = {
        "bground_im": bground_im,
        "roi": roi,
        "first_frame": first_frame,
        "first_frame_idx": first_frame_idx,
        "last_frame_idx": last_frame_idx,
        "nframes": total_frames,
        "frame_batches": frame_batches,
    }

    # farm out the batches and write to an hdf5 file
    with h5py.File(results_filename, "w") as f:
        # Write scalars, roi, acquisition metadata, etc. to h5 file
        create_extract_h5(
            **extraction_data,
            h5_file=f,
            acquisition_metadata=acquisition_metadata,
            config_data=config_data,
            status_dict=status_dict,
            scalars_attrs=scalars_attrs,
        )

        # Write crop-rotated results to h5 file and write video preview mp4 file
        process_extract_batches(
            **extraction_data,
            h5_file=f,
            input_file=input_file,
            config_data=config_data,
            scalars=scalars,
            # str_els=str_els,
            output_mov_path=movie_filename,
        )

    print()

    status_dict["complete"] = True
    if status_dict["parameters"].get("true_depth") is None:
        status_dict["parameters"]["true_depth"] = float(config_data.get("true_depth"))
    with open(status_filename, "w") as f:
        yaml.dump(status_dict, f)

    return output_dir


# one stop shopping for taking some frames and doing stuff
def extract_chunk(
    chunk,
    tail_ksize=15,
    dilate=True,
    dilation_ksize=5,
    min_height=10,
    max_height=300,
    use_cc=False,
    bground=None,
    flip_classifier=None,
    flip_classifier_smoothing=51,
    progress_bar=True,
    crop_size=(256, 256),
    true_depth=950,
    compute_raw_scalars=False,
    sam2_checkpoint=None,
    sam2_points=None,
    **kwargs,
):
    """
    Extract mouse from the depth videos.

    Args:
    chunk (np.ndarray): chunk to extract - (chunksize, height, width)
    use_tracking_model (bool): The EM tracker uses expectation-maximization to fit improve mouse detection.
    spatial_filter_size (tuple): spatial kernel size used in median filtering.
    temporal_filter_size (tuple): temporal kernel size used in median filtering.
    tail_filter_iters (int): number of filtering iterations on mouse tail
    iters_min (int): minimum tail filtering filter kernel size
    strel_tail (cv2::StructuringElement): filtering kernel size to filter out mouse tail.
    strel_min (cv2::StructuringElement): filtering kernel size to filter mouse body in cable recording cases.
    min_height (int): minimum (mm) distance of mouse to floor.
    max_height (int): maximum (mm) distance of mouse to floor.
    mask_threshold (int): Threshold on log-likelihood to include pixels for centroid and angle calculation
    use_cc (bool): boolean to use connected components in cv2 structuring elements
    bground (np.ndarray): 2D numpy array representing previously computed median background image of entire extracted recording.
    roi (np.ndarray): 2D numpy array representing previously computed roi (area of bucket floor) to search for mouse within.
    flip_classifier (str): path to pre-selected flip classifier.
    flip_classifier_smoothing (int): amount of smoothing to use for flip classifier.
    save_path: (str): Path to save extracted results
    progress_bar (bool): Display progress bar
    crop_size (tuple): size of the cropped mouse image.
    true_depth (float): the computed detected true depth value for the middle of the arena
    model_smoothing_clips (tuple): Model smoothing clips
    tracking_model_init (str): Method for tracking model initialization
    compute_raw_scalars (bool): Compute scalars from unfiltered crop-rotated data.

    Returns:
    results (dict): dict object containing the following keys:
    chunk (numpy.ndarray): bg subtracted and applied ROI version of original video chunk
    depth_frames(numpy.ndarray): cropped and oriented mouse video chunk
    mask_frames (numpy.ndarray): cropped and oriented mouse video chunk
    scalars (dict): computed scalars (str) mapped to 1d numpy arrays of length=nframes.
    flips(1d array): list of frame indices where the mouse orientation was flipped.
    parameters (dict): mean and covariance estimates for each frame (if em_tracking=True), otherwise None.
    """

    if bground is not None:
        chunk = (bground - chunk).astype(chunk.dtype)
        # Threshold chunk depth values at min and max heights
        chunk = threshold_chunk(chunk, min_height, max_height).astype(int)

    # pack clean params into a dict
    clean_params = {
        'tail_ksize': tail_ksize,
        'dilate': dilate,
        'dilation_ksize': dilation_ksize
    }

    # get the sam2 predictor
    predictor = get_sam2_predictor(sam2_checkpoint)
    # TODO if somehow detect centroid if not DLC keypoints

    # get masks from sam2
    masks, _ = segment_chunk(
        chunk, predictor, sam2_points, clean_params, inference_state=None
    )
    # apply masks to chunk
    chunk = chunk * masks
    
    # now get the centroid and orientation of the mouse
    features = get_frame_features(
        chunk,
        frame_threshold=min_height,
        use_cc=use_cc,
        progress_bar=progress_bar,
    )

    incl = ~np.isnan(features["orientation"])
    features["orientation"][incl] = np.unwrap(features["orientation"][incl] * 2) / 2

    # Crop and rotate the original frames
    cropped_frames = crop_and_rotate_frames(
        chunk, features, crop_size=crop_size, progress_bar=progress_bar
    )

    # Crop and rotate the filtered frames to be returned and later written
    cropped_filtered_frames = crop_and_rotate_frames(
        chunk, features, crop_size=crop_size, progress_bar=progress_bar
    )

    masks = crop_and_rotate_frames(
        masks, features, crop_size=crop_size, progress_bar=progress_bar
    )

    # Orient mouse to face east
    if flip_classifier:
        # get frame indices of incorrectly orientation
        flips = get_flips(
            cropped_filtered_frames, flip_classifier, flip_classifier_smoothing
        )
        flip_indices = np.where(flips)

        # apply flips
        cropped_frames[flip_indices] = np.rot90(
            cropped_frames[flip_indices], k=2, axes=(1, 2)
        )
        cropped_filtered_frames[flip_indices] = np.rot90(
            cropped_filtered_frames[flip_indices], k=2, axes=(1, 2)
        )
        masks[flip_indices] = np.rot90(masks[flip_indices], k=2, axes=(1, 2))
        features["orientation"][flips] += np.pi

    else:
        flips = None

    if compute_raw_scalars:
        # Computing scalars from raw data
        scalars = compute_scalars(
            cropped_frames,
            features,
            min_height=min_height,
            max_height=max_height,
            true_depth=true_depth,
        )
    else:
        # Computing scalars from filtered data
        scalars = compute_scalars(
            cropped_filtered_frames,
            features,
            min_height=min_height,
            max_height=max_height,
            true_depth=true_depth,
        )

    # Store all results in a dictionary
    results = {
        "chunk": chunk,
        "depth_frames": cropped_frames,
        "mask_frames": masks,
        "scalars": scalars,
        "flips": flips,
    }

    return results


@filter_warnings
def run_local_batch_extract(
    to_extract, config_file, num_frames=None, skip_extracted=False
):
    """
    Run the extract command on given list of sessions to extract on a local platform.

    Args:
    to_extract (list): list of paths to files to extract
    config_file (str): path to configuration file containing pre-configured extract and ROI
    skip_extracted (bool): Whether to skip already extracted session.

    """

    for input_file in tqdm(to_extract, desc="Extracting Sessions"):
        try:
            config_data = read_yaml(config_file)

            # Loading individual session config parameters if it exists
            if exists(config_data.get("session_config_path", "")):
                session_configs = read_yaml(config_data["session_config_path"])
                session_key = basename(dirname(input_file))

                # If key is found, update config_data, otherwise, use default dict
                config_data = session_configs.get(session_key, config_data)

            if output_dir is None:
                output_dir = config_data.get("output_dir", "proc")

            run_extraction(
                input_file,
                output_dir,
                config_data,
                num_frames=num_frames,
                skip=skip_extracted,
            )

        except Exception as e:
            print("Unexpected error:", e)
            print("could not extract", input_file)


def run_slurm_batch_extract(input_dir, to_extract, config_data, skip_extracted=False):

    assert (
        "extract_out_script" in config_data
    ), "Need to supply extract_out_script to save extract commands"
    # expand input_dir absolute path
    input_dir = abspath(input_dir)

    # make session_specific config file and save it in proc folder if session config exists
    if exists(config_data.get("session_config_path", "")):
        session_configs = read_yaml(config_data["session_config_path"])
        for depth_file in to_extract:
            output_dir = join(dirname(depth_file), config_data["output_dir"])

            # ensure output_dir exists
            if not exists(output_dir):
                makedirs(output_dir)

            # get and write session-specific parameters
            output_file = join(output_dir, "config.yaml")
            session_key = basename(dirname(depth_file))

            with open(output_file, "w") as f:
                yaml.dump(session_configs.get(session_key, config_data), f)

    # Construct sbatch command for slurm
    commands = ""
    for depth_file in to_extract:
        output_dir = join(dirname(depth_file), config_data["output_dir"])

        # skip session if skip_extracted is true and the session is already extracted
        if skip_extracted and check_completion_status(
            join(output_dir, "results_00.yaml")
        ):
            continue

        # set up config file
        if exists(config_data.get("session_config_path", "")):
            config_file = join(output_dir, "config.yaml")
        else:
            config_file = config_data["config_file"]

        # construct command
        base_command = (
            f'moseq2-extract extract --config-file {config_file} {depth_file}; "\n'
        )
        prefix = f'sbatch -c {config_data["ncpus"] if config_data["ncpus"] > 0 else 1} --mem={config_data["memory"]} '
        prefix += f'-p {config_data["partition"]} -t {config_data["wall_time"]} --wrap "{config_data["prefix"]}'
        commands += prefix + base_command

    # Ensure output directory exists
    config_data["extract_out_script"] = join(
        input_dir, config_data["extract_out_script"]
    )
    with open(config_data["extract_out_script"], "w") as f:
        f.write(commands)
    print("Commands saved to:", config_data["extract_out_script"])

    # Print command
    if config_data["get_cmd"]:
        print("Listing extract commands...\n")
        print(commands)

    # Run command using system
    if config_data["run_cmd"]:
        print("Running extract commands")
        system(commands)
